{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNmrLB/mQXDZvxWbPCnnELY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rudr16a/GitHub-Fine-tuned-/blob/main/Colab_github_Fine_Tuned_Diffusion_Demo_from_HF_Spaces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh5aEbkB9WTa",
        "outputId": "623e4387-68ea-4553-b47a-7fbe4f540ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul 21 15:43:22 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\"GPU 🔥\" if torch.cuda.is_available() else \"CPU 🥶\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V_yl6xEQ9grk",
        "outputId": "789f8aa1-1ebe-4e76-c36a-21a6566ff113"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GPU 🔥'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://huggingface.co/spaces/anzorq/finetuned_diffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciOfalty9twt",
        "outputId": "4f1e0e12-fea5-48ba-bac8-7bcac4d3d169"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'finetuned_diffusion'...\n",
            "remote: Enumerating objects: 364, done.\u001b[K\n",
            "remote: Counting objects: 100% (364/364), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 364 (delta 228), reused 364 (delta 228), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (364/364), 129.01 KiB | 614.00 KiB/s, done.\n",
            "Resolving deltas: 100% (228/228), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd finetuned_diffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf_xNY1_9xWs",
        "outputId": "0c2779ad-5804-48f6-8596-45f0cc153537"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetuned_diffusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EYrtvIe-KLq",
        "outputId": "949b4480-d25f-4f15-d2fb-c99dca95e613"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tnsfw.png  README.md  requirements.txt  style.css  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27vA3He--PZX",
        "outputId": "cdda4614-b491-4630-824b-d3d52e66e7f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egualWJ--XWf",
        "outputId": "29ff29a3-b592-4796-b82e-0a481ae1f2a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from email import generator\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "models = [\n",
        "  \"nitrosocke/Arcane-Diffusion\",\n",
        "  \"nitrosocke/archer-diffusion\",\n",
        "  \"nitrosocke/elden-ring-diffusion\",\n",
        "  \"nitrosocke/spider-verse-diffusion\",\n",
        "  \"nitrosocke/modern-disney-diffusion\",\n",
        "  \"hakurei/waifu-diffusion\",\n",
        "  \"lambdalabs/sd-pokemon-diffusers\",\n",
        "  \"yuk/fuyuko-waifu-diffusion\",\n",
        "  \"AstraliteHeart/pony-diffusion\",\n",
        "  \"IfanSnek/JohnDiffusion\",\n",
        "  \"nousr/robo-diffusion\",\n",
        "  \"DGSpitzer/Cyberpunk-Anime-Diffusion\"\n",
        "]\n",
        "\n",
        "prompt_prefixes = {\n",
        "  models[0]: \"arcane style \",\n",
        "  models[1]: \"archer style \",\n",
        "  models[2]: \"elden ring style \",\n",
        "  models[3]: \"spiderverse style \",\n",
        "  models[4]: \"modern disney style \",\n",
        "  models[5]: \"\",\n",
        "  models[6]: \"\",\n",
        "  models[7]: \"\",\n",
        "  models[8]: \"\",\n",
        "  models[9]: \"\",\n",
        "  models[10]: \"\",\n",
        "  models[11]: \"dgs illustration style \",\n",
        "}\n",
        "\n",
        "current_model = models[0]\n",
        "pipe = StableDiffusionPipeline.from_pretrained(current_model, torch_dtype=torch.float16)\n",
        "if torch.cuda.is_available():\n",
        "  pipe = pipe.to(\"cuda\")\n",
        "\n",
        "device = \"GPU 🔥\" if torch.cuda.is_available() else \"CPU 🥶\"\n",
        "\n",
        "def inference(model, img, strength, prompt, guidance, steps, seed):\n",
        "\n",
        "  generator = torch.Generator('cuda').manual_seed(seed) if seed != 0 else None\n",
        "\n",
        "  if img is not None:\n",
        "    return img_inference(model, prompt, img, strength, guidance, steps, generator)\n",
        "  else:\n",
        "    return text_inference(model, prompt, guidance, steps, generator)\n",
        "\n",
        "def text_inference(model, prompt, guidance, steps, generator=None):\n",
        "\n",
        "    global current_model\n",
        "    global pipe\n",
        "    if model != current_model:\n",
        "        current_model = model\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(current_model, torch_dtype=torch.float16)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            pipe = pipe.to(\"cuda\")\n",
        "\n",
        "    prompt = prompt_prefixes[current_model] + prompt\n",
        "    image = pipe(\n",
        "      prompt,\n",
        "      num_inference_steps=int(steps),\n",
        "      guidance_scale=guidance,\n",
        "      width=512,\n",
        "      height=512,\n",
        "      generator=generator).images[0]\n",
        "    return image\n",
        "\n",
        "def img_inference(model, prompt, img, strength, guidance, steps, generator):\n",
        "\n",
        "    global current_model\n",
        "    global pipe\n",
        "    if model != current_model:\n",
        "        current_model = model\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(current_model, torch_dtype=torch.float16)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            pipe = pipe.to(\"cuda\")\n",
        "\n",
        "    prompt = prompt_prefixes[current_model] + prompt\n",
        "    img.resize((512, 512))\n",
        "    image = pipe(\n",
        "        prompt,\n",
        "        init_image=img,\n",
        "        num_inference_steps=int(steps),\n",
        "        strength=strength,\n",
        "        guidance_scale=guidance,\n",
        "        width=512,\n",
        "        height=512,\n",
        "        generator=generator).images[0]\n",
        "    return image\n",
        "\n",
        "\n",
        "css = \"\"\"\n",
        "  <style>\n",
        "  .finetuned-diffusion-div {\n",
        "      text-align: center;\n",
        "      max-width: 700px;\n",
        "      margin: 0 auto;\n",
        "    }\n",
        "    .finetuned-diffusion-div div {\n",
        "      display: inline-flex;\n",
        "      align-items: center;\n",
        "      gap: 0.8rem;\n",
        "      font-size: 1.75rem;\n",
        "    }\n",
        "    .finetuned-diffusion-div div h1 {\n",
        "      font-weight: 900;\n",
        "      margin-bottom: 7px;\n",
        "    }\n",
        "    .finetuned-diffusion-div p {\n",
        "      margin-bottom: 10px;\n",
        "      font-size: 94%;\n",
        "    }\n",
        "    .finetuned-diffusion-div p a {\n",
        "      text-decoration: underline;\n",
        "    }\n",
        "  </style>\n",
        "\"\"\"\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "            <div class=\"finetuned-diffusion-div\">\n",
        "              <div>\n",
        "                <h1>Finetuned Diffusion</h1>\n",
        "              </div>\n",
        "              <p>\n",
        "               Demo for multiple fine-tuned Stable Diffusion models, trained on different styles: <br>\n",
        "               <a href=\"https://huggingface.co/nitrosocke/Arcane-Diffusion\">Arcane</a>, <a href=\"https://huggingface.co/nitrosocke/archer-diffusion\">Archer</a>, <a href=\"https://huggingface.co/nitrosocke/elden-ring-diffusion\">Elden Ring</a>, <a href=\"https://huggingface.co/nitrosocke/spider-verse-diffusion\">Spiderverse</a>, <a href=\"https://huggingface.co/nitrosocke/modern-disney-diffusion\">Modern Disney</a>, <a href=\"https://huggingface.co/hakurei/waifu-diffusion\">Waifu</a>, <a href=\"https://huggingface.co/lambdalabs/sd-pokemon-diffusers\">Pokemon</a>, <a href=\"https://huggingface.co/yuk/fuyuko-waifu-diffusion\">Fuyuko Waifu</a>, <a href=\"https://huggingface.co/AstraliteHeart/pony-diffusion\">Pony</a>, <a href=\"https://huggingface.co/IfanSnek/JohnDiffusion\">John</a>, <a href=\"https://huggingface.co/nousr/robo-diffusion\">Robo</a>, <a href=\"https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion\">Cyberpunk Anime</a>\n",
        "              </p>\n",
        "            </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "\n",
        "        with gr.Column():\n",
        "\n",
        "            model = gr.Dropdown(label=\"Model\", choices=models, value=models[0])\n",
        "            prompt = gr.Textbox(label=\"Prompt\", placeholder=\"Style prefix is applied automatically\")\n",
        "            with gr.Accordion(\"Image to image (optional)\", open=False):\n",
        "              image = gr.Image(label=\"Image\", height=256, tool=\"editor\", type=\"pil\")\n",
        "              strength = gr.Slider(label=\"Strength\", minimum=0, maximum=1, step=0.01, value=0.75)\n",
        "\n",
        "            with gr.Accordion(\"Advanced options\", open=False):\n",
        "              guidance = gr.Slider(label=\"Guidance scale\", value=7.5, maximum=15)\n",
        "              steps = gr.Slider(label=\"Steps\", value=50, maximum=100, minimum=2)\n",
        "              seed = gr.Slider(0, 2147483647, label='Seed (0 = random)', value=0, step=1)\n",
        "\n",
        "            run = gr.Button(value=\"Run\")\n",
        "            gr.Markdown(f\"Running on: {device}\")\n",
        "        with gr.Column():\n",
        "            image_out = gr.Image(height=512)\n",
        "\n",
        "    prompt.submit(inference, inputs=[model, image, strength, prompt, guidance, steps, seed], outputs=image_out)\n",
        "    run.click(inference, inputs=[model, image, strength, prompt, guidance, steps, seed], outputs=image_out)\n",
        "    gr.Examples([\n",
        "        [models[0], \"jason bateman disassembling the demon core\", 7.5, 50],\n",
        "        [models[3], \"portrait of dwayne johnson\", 7.0, 75],\n",
        "        [models[4], \"portrait of a beautiful alyx vance half life\", 10, 50],\n",
        "        [models[5], \"Aloy from Horizon: Zero Dawn, half body portrait, smooth, detailed armor, beautiful face, illustration\", 7, 45],\n",
        "        [models[4], \"fantasy portrait painting, digital art\", 4, 30],\n",
        "    ], [model, prompt, guidance, steps], image_out, text_inference, cache_examples=torch.cuda.is_available())\n",
        "    gr.Markdown('''\n",
        "      Models by [@nitrosocke](https://huggingface.co/nitrosocke), [@Helixngc7293](https://twitter.com/DGSpitzer) and others. ❤️<br>\n",
        "      Space by: [![Twitter Follow](https://img.shields.io/twitter/follow/hahahahohohe?label=%40anzorq&style=social)](https://twitter.com/hahahahohohe)\n",
        "\n",
        "      ![visitors](https://visitor-badge.glitch.me/badge?page_id=anzorq.finetuned_diffusion)\n",
        "    ''')\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(share = True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iURJltgD-cYv",
        "outputId": "fe1d7238-a516-4a90-c4b8-6ed243f08343"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBJqIGDy_A9c",
        "outputId": "a19524b2-2bda-4de3-d9b6-59a2194e54c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/finetuned_diffusion/app.py\", line 2, in <module>\n",
            "    from diffusers import StableDiffusionPipeline\n",
            "ModuleNotFoundError: No module named 'diffusers'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aTpv5pcq_Hvu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}